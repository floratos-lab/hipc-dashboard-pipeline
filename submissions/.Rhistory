df2$target_pathogen[w] <- "Mycobacterium bovis BCG"
# fix capitalization (should fix in source google sheet)
df2$response_behavior_type <- tolower(df2$response_behavior_type)
df2$response_behavior      <- tolower(df2$response_behavior)
# check if  comment column has more than 255 character limit
w <- sapply(df2$comments[1:10], function(x) {nchar(x) > 255}, USE.NAMES	= FALSE)
if(any(!is.na(w)) && any(w)) {
stop(paste("comment too long for row(s)", paste(df2$row_key[w], collapse = ", ")))
}
# check if comment column has non-UTF8 characters
w <- !stri_enc_isutf8(df2$comments)
if(any(w)) {
stop(paste("comment has non-utf8 character in row(s)", paste(df2$row_key[w], collapse = ", ")))
}
# We don't expect non-ascii characters in text version of signature_source
w <- !stri_enc_isascii(df2$signature_source)
if(any(w)) {
stop(paste("signature_source has non-ascii character in row(s)", paste(df2$row_key[w], collapse = ", ")))
}
##########################################################
##### Other global changes to template before splits #####
##########################################################
## Create a map of VO codes to text equivalents
## Both columns must have the same number of values per row
codes <- strsplit(df2$exposure_material,";")  # returns a list
text  <- strsplit(df2$exposure_material_text, ";") # returns a list
m <- mapply(function(x, y) {length(x) == length(y)}, codes, text)
if (!all(m)) {
# FIXME - should just stop, no reason to continue
warning(paste("exposure code vs text length mismatch for rows rowkeys:", paste(df2$row_key[!m], collapse = ", ")))
warning(paste("removing non-matching rows"))
df2 <- df2[m, ]
codes <- codes[m]
text <- text[m]
}
codes <- trimws(unlist(codes))
text <- trimws(unlist(text))
vo_code_map <- unique(data.frame(codes, text, stringsAsFactors = FALSE))
vo_code_map <- vo_code_map[order(vo_code_map$codes), ]
write.table(vo_code_map,
file = logfile_path(logdir, base_filename, "VO_code_mapping.txt"),
sep = "\t", row.names = FALSE, col.names = FALSE)
dupcodes <- vo_code_map[duplicated(vo_code_map$codes), ]$codes
w <- which(vo_code_map$codes %in% dupcodes)
write.table(vo_code_map[w, ],
file = logfile_path(logdir, base_filename, "VO_code_duplicates.txt"),
sep = "\t", row.names = FALSE, col.names = FALSE)
############################################################################
##### Data substitution and splitting (1): response_component_original #####
############################################################################
if (sheet_type == "GENE") {
# df2$response_component_original values become factors!
df2 <- cSplit(df2, "response_component_original", sep = ",", direction = "long")
df2 <- cSplit(df2, "response_component_original", sep = ";", direction = "long") # ";" not used so far
affyHits <- grep("///", df2$response_component_original)
write.csv(df2[affyHits,c("response_component_original", "publication_reference", "subm_obs_id")],
file = logfile_path(logdir, base_filename, "affyHits.csv"),
row.names = FALSE)
# using " /// " leaves extra slashes, regexp "[/]{3}" does not
df2 <- cSplit(df2, "response_component_original", sep = "[/]{3}", direction = "long", fixed = FALSE)
# the curated data does in places have spaces as separators
df2 <- cSplit(df2, "response_component_original", sep = " ", direction = "long")
} else if (sheet_type == "CELLTYPE_FREQUENCY") {
df2 <- cSplit(df2, "response_component_original", sep = ";", direction = "long")
}
# The cSplit() calls produce a data.table of data.frame rows.  Coerce back to just data.frame.
df2 <- as.data.frame(df2)
# remove factors from df2$response component so that for genes, can alter values.
# FIXME - any way to gain better control of this?
df2$response_component_original <- as.character(df2$response_component_original)
# create original gene symbols "signature" list after applying manual corrections
# FIXME - need better variable name than "signatures"
if (sheet_type == "GENE") {
# Apply manual gene corrections
rvl <- manual_gene_corrections(df2$response_component_original,
paste(source_data_dir, manual_gene_corrections_file, sep = "/"))
genes <- rvl$genes
summary_df <- rbind(summary_df, rvl$summary)
# Fix certain gene symbols containing "orf"
genes <- fix_orf_symbols(genes)
# Reconstruct original signatures, after splitting by various separators
signatures <- lapply(unique(df2$uniq_obs_id), function(uniqID) {
genes[df2$uniq_obs_id == uniqID]
})
} else if (sheet_type == "CELLTYPE_FREQUENCY") {
# Reconstruct original signatures, after splitting by various separators
signatures <- lapply(unique(df2$uniq_obs_id), function(uniqID) {
df2$response_component_original[df2$uniq_obs_id == uniqID]
})
}
# list of original response components before main changes, with duplicates per signature removed
signatures_uniq <- lapply(signatures, unique)
# name for indexing into signatures_uniq
names(signatures_uniq) <- unique(df2$uniq_obs_id)
uids_list <- unique(df2$uniq_obs_id)
# get original count of rows for each uniq_obs_id.
uids_cnt  <- sapply(uids_list, function(x) {sum(df2$uniq_obs_id == x)})
for (i in 1:length(uids_list)) {
# NOTE - df2$response_comp_orig_cnt is of type "character".  Be careful!
df2$response_comp_orig_cnt[df2$uniq_obs_id == uids_list[i]] <- uids_cnt[i]
}
changed <- mapply(function(x, n) {length(x) != length(n)}, signatures, signatures_uniq)
if (length(changed) > 0) {
print(sum(changed))
}
# if want to see that actual number of changes per signature
# tt <- mapply(function(x, n) {if(length(x) != length(n)) {
#                              print(paste(length(x), length(n), length(x) - length(n)))
#                             }}, signatures, signatures_uniq)
# find all duplicate symbols per signature
dups <- lapply(signatures, function(x) {unique(x[duplicated(x)])})
if(length(dups) > 0) {
# assign each dup its row_key as name
names(dups) <- sapply(unique(df2$uniq_obs_id), function(x) {
unique(df2[df2$uniq_obs_id == x, "row_key"])
})
# only keep entries that have duplicates
dup_resp_comps <- dups[sapply(dups, function(x) {length(x) > 0})]
# Duplicate symbols can appear e.g. when probesets are translated to gene symbols
summary_df <- add_to_summary(summary_df, "Number of signatures with duplicated items", length(dup_resp_comps))
summary_df <- add_to_summary(summary_df, "Total number of unique duplicated items", length(unlist(dup_resp_comps)))
# Write out list of counts of duplicated response components by signature
write.table(paste(names(dup_resp_comps), sapply(dup_resp_comps, length), sep = ", "),
file = logfile_path(logdir, base_filename, "response_component_duplicates_count.csv"),
row.names = FALSE, col.names = FALSE, quote = FALSE)
# Write out list of duplicated response components by signature
# Have to write in loop because varying number of elements per row
outfile = logfile_path(logdir, base_filename, "response_component_duplicates.txt")
if(file.exists(outfile)) file.remove(outfile)
d <- mapply(function(x, n) {
write.table(paste(c(x, n), collapse = "\t"),
file = outfile,
row.names = FALSE, col.names = FALSE, quote = FALSE, append = TRUE)
}, names(dup_resp_comps),  dup_resp_comps)
# Write out sorted list of duplicated response components by signature
outfile = logfile_path(logdir, base_filename, "response_component_duplicates_sorted.txt")
if(file.exists(outfile)) file.remove(outfile)
d <- mapply(function(x, n) {
write.table(paste(c(x, sort(n)), collapse = "\t"),
file = outfile,
row.names = FALSE, col.names = FALSE, quote = FALSE, append = TRUE)
}, names(dup_resp_comps), dup_resp_comps)
}
start_cnt <- nrow(df2)
# Update original gene symbols to latest versions (NCBI/HGNC)
if (sheet_type == "GENE") {
# starts with "genes" manually corrected list from above
# make sure "genes" is still in synch with df2.
length(genes) == nrow(df2)
rvl <- update_gene_symbols(genes, logdir, base_filename,
source_data_dir,
DOWNLOAD_NEW_HGNC)
genes_map <- rvl$genes_map
summary_df <- rbind(summary_df, rvl$summary)
rm(rvl)
# w <- which(genes_map$Symbol != genes_map$alias)
# Save the gene map for the unmatched symbols with their uniq_obs_id
# to add them back to complete signatures later
unmatched_symbols_map <- cbind(genes_map, uniq_obs_id = df2$uniq_obs_id)
unmatched_symbols_map <- unmatched_symbols_map[is.na(unmatched_symbols_map$Symbol), ]
# copy the fixed symbols back to the main data structure df2
df2$response_component <- genes_map$Symbol
# get rid of genes that had no valid symbol.
df2 <- df2[!is.na(df2$response_component), ]
# Remove genes that are OK in HGNC but fail at NCBI
if(exists("ncbi_no_symbol")) {
df2 <- df2[!(df2$response_component %in% ncbi_no_symbol), ]
}
# Fix symbols where NCBI is not using current HGNC symbol
if(exists("ncbi_fixes")) {
for(i in 1:nrow(ncbi_fixes)) {
df2$response_component[df2$response_component == ncbi_fixes$hgnc[i]] <- ncbi_fixes$ncbi[i]
}
}
summary_df <- add_to_summary(summary_df,
"Number of valid gene symbols" ,
length(unique(df2$response_component)))
} else if (sheet_type == "CELLTYPE_FREQUENCY") {
# ctf_match will have NA values where no match found
ctf_match <- match(df2$response_component_original, ctf_fixes$original_annotation)
# Test if any rows in cell-type mapping sheet are unused.
w <- !(1:length(ctf_fixes$original_annotation) %in% unique(ctf_match))
if(any(w)) {
print("not all cell-type mappings used:")
print(sapply(ctf_fixes$original_annotation[w], function(x) {which(ctf_fixes$original_annotation == x) + 1}))
} else {
print("all cell-type mappings used")
}
if (any(is.na(ctf_match))) {
w <- which(is.na(ctf_match))
print(paste("row", df2$uniq_obs_id[w], ", no mapping for cell type",
df2$response_component_original[w]))
missing_mappings <- data.frame(row = df2$uniq_obs_id[w], label = df2$response_component_original[w])
write.xlsx(missing_mappings,
file = logfile_path(logdir, base_filename, "missing_mappings.xlsx"),
sheetName = "all rows", append = FALSE, row.names = FALSE)
write.xlsx(unique(sort(df2$response_component_original[w])),
file = logfile_path(logdir, base_filename, "missing_mappings.xlsx"),
sheetName = "unique", append = TRUE, row.names = FALSE)
stop("cell type mapping not found")
} else {
print("all cell types matched")
}
failed_matches <- df2$response_component_original[is.na(ctf_match)]
successful_matches <- df2$response_component_original[!is.na(ctf_match)]
summary_df <- add_to_summary(summary_df, "Unique failed cell type tag matches",
length(unique(failed_matches)))
summary_df <- add_to_summary(summary_df, "Unique successful cell type tag matches" ,
length(unique(successful_matches)))
if (length(failed_matches) > 0) {
write.table(unique(failed_matches),
file = logfile_path(logdir, base_filename, "no_cell_type_match.txt"),
sep = "\t", row.names = FALSE, col.names = FALSE, quote = FALSE)
write.xlsx(unique(failed_matches),
file = logfile_path(logdir, base_filename, "no_cell_type_match.xlsx"),
row.names = FALSE, col.names = FALSE)
write.table(sort(failed_matches_uniq),
file = logfile_path(logdir, base_filename, "no_cell_type_match_sorted.txt"),
sep = "\t", row.names = FALSE, col.names = FALSE, quote = FALSE)
write.xlsx(sort(failed_matches_uniq),
file = logfile_path(logdir, base_filename, "no_cell_type_match_sorted.xlsx"),
row.names = FALSE, col.names = FALSE)
}
# combine proterm(s) and extra together.  All same length as df2.
proterm <- ctf_fixes$PRO_term_w_intensity[ctf_match]
extra <- ctf_fixes$extra[ctf_match]
# vector initialized with ""
proterm_and_extra <- vector(mode = "character", length = length(proterm))
for (i in 1:length(proterm)) {
if(is.na(ctf_match[i])) {
# should not happen, already checked
print(paste("no mapping for cell type at index", i))
next
}
# insert comma between non-empty terms
a <- c(proterm[i], extra[i])
a <- a[a!=""]                   # returns char(0) if both empty
a <- paste(a, collapse = ", ")  # returns "" on empty
proterm_and_extra[i] <- ifelse(a != "", paste("&", a), "")
}
df2$response_component <- ctf_fixes$CL_term[ctf_match]
df2$cell_ontology_id   <- ctf_fixes$CL_ID[ctf_match]
df2$proterm_and_extra  <- proterm_and_extra
df2$pro_ontology_id    <- ctf_fixes$PRO_ID[ctf_match]
df2$fully_qualified_response_component <- ""
# Remove any unmatched cell types (should have already been caught)
df2 <- df2[!is.na(ctf_match), ]
summary_df <- add_to_summary(summary_df,
"Unique cell types after substitutions",
length(unique(df2$response_component)))
# remove trailing white space if proterm_and_extra was empty
concatenated_cell_types <- trimws(paste(df2$response_component, df2$proterm_and_extra, sep = " "))
df2$fully_qualified_response_component <- concatenated_cell_types
concatenated_cell_types <- sort(unique(concatenated_cell_types))
write.table(concatenated_cell_types,
file = logfile_path(logdir, base_filename, "concatenated_cell_types.txt"),
sep = "\t", row.names = FALSE, col.names = FALSE, quote = FALSE)
summary_df <- add_to_summary(summary_df,
"Unique cell type + marker combinations",
length(concatenated_cell_types))
}
end_cnt <- nrow(df2)
summary_df <- add_to_summary(summary_df,
"Split 1, first reduction, rows lost to unknown response component",
start_cnt - end_cnt)
# Remove exact duplicates (implies duplicate copies of response component in a signature)
# This can happen when e.g. two or more original probesets get map to one symbol.
start_cnt <- end_cnt
df2 <- unique(df2)
end_cnt <- nrow(df2)
summary_df <- add_to_summary(summary_df, "Split 1, rows before removal" , start_cnt)
summary_df <- add_to_summary(summary_df, "Split 1, rows with non-unique response component removed" , start_cnt - end_cnt)
summary_df <- add_to_summary(summary_df, "Split 1, rows remaining", end_cnt)
start_cnt <- end_cnt
# get count of rows for each uniq_obs_id.
uids_list <- unique(df2$uniq_obs_id)
uids_cnt  <- sapply(uids_list, function(x) {sum(df2$uniq_obs_id == x)})
for (i in 1:length(uids_list)) {
df2$response_comp_cnt[df2$uniq_obs_id == uids_list[i]] <- uids_cnt[i]
}
start_cnt <- end_cnt
write_unique_list(df2$response_component, logdir, base_filename, "response_component_list")
#################################################
##### Data splitting (2): exposure material #####
#################################################
# Only gene and cell type frequency have data of this type so far
if (sheet_type == "CELLTYPE_FREQUENCY" | sheet_type == "GENE") {
write_unique_list(df2$exposure_material_text, logdir, base_filename, "exposure_material_text", do_split = TRUE)
df2 <- cSplit(df2, "exposure_material", sep = ";", direction = "long")
write_unique_list(df2$exposure_material, logdir, base_filename, "exposure_material", do_split = FALSE)
}
end_cnt <- nrow(df2)
summary_df <- add_to_summary(summary_df, "Split 2, on exposure material, rows added", end_cnt - start_cnt)
summary_df <- add_to_summary(summary_df, "Split 2, on exposure material, total rows", end_cnt)
summary_df <- add_to_summary(summary_df, "unique VO codes in exposure_material", length(unique(df2$exposure_material)))
start_cnt <- end_cnt
# The cSplit() calls produce a data.table of data.frame rows.  Coerce back to just data.frame.
df2 <- as.data.frame(df2, stringsAsFactors = FALSE)
# Substitute in actual virus components for influenza pathogens only.
for (i in 1:nrow(df2)) {
if( (df2[i, "vaccine_year"] != "" ) || (df2[i, "exposure_material"] == "VO_0000410")) {
df2[i, "target_pathogen"] <- lookup_vaccine(as.character(df2[i, "vaccine_year"]), as.character(df2[i, "exposure_material"]))
}
}
# Substitute in actual pathogens for measles + DPT vaccination
# Note - the specific pathogen strains below are not specified by the VO codes used.
# These are the only case in Gene and Cell-type where there are more than a single pathogen,
# other than for influenza, so have to rewrite the pathogen entries to match split vaccine VO codes
w <- which(df2$exposure_material == "VO_0000654")
df2[w, "target_pathogen"] <- "Measles virus strain Edmonston-Zagreb"
w <- which(df2$exposure_material == "VO_0000738")
df2[w, "target_pathogen"] <- "Corynebacterium diphtheriae; Clostridium tetani; Human poliovirus 1; Human poliovirus 2; Human poliovirus 3"
is.factor(df2$exposure_material)
#########################################
##### Data splitting (3): Pathogens #####
#########################################
# split
df2 <- cSplit(df2, "target_pathogen", sep = ";", direction = "long")
# class(df2)  # "data.table" "data.frame"
df2 <- as.data.frame(df2)
end_cnt <- nrow(df2)
summary_df <- add_to_summary(summary_df, "Split 3, on pathogens, rows added", end_cnt - start_cnt)
summary_df <- add_to_summary(summary_df, "Split 3, on pathogens, total rows", end_cnt)
start_cnt <- end_cnt
# Write unique list of pathogens after substitutions
write_unique_list(df2$target_pathogen, logdir, base_filename, "target_pathogens_after_fixes")
summary_df <- add_to_summary(summary_df, "target_pathogens_after_fixes", length(unique(df2$target_pathogen)))
# Now substitute for viral strains missing in NCBI Taxonomy
df2$target_pathogen <- sub("Influenza A virus \\(A/Switzerland/9715293/2013\\(H3N2\\)\\)", "H3N2 subtype", df2$target_pathogen)
df2$target_pathogen <- sub("Influenza B virus \\(B/Phuket/3073/2013\\)", "Influenza B virus", df2$target_pathogen)
# lowercase first letter of each pathogen
substr(df2$target_pathogen, 1, 1) <- tolower(substr(df2$target_pathogen, 1,1))
# Write unique list of pathogens after final substitutions
write_unique_list(df2$target_pathogen, logdir, base_filename, "target_pathogens_after_final_fixes")
summary_df <- add_to_summary(summary_df, "target_pathogens_after_final_fixes", length(unique(df2$target_pathogen)))
# Write unique list of tissue types
write_unique_list(df2$tissue_type, logdir, base_filename, "tissue_type_list")
summary_df <- add_to_summary(summary_df, "tissue types", length(unique(df2$tissue_type)))
####################################################################
#### Get counts of each response component for e.g. word cloud #####
####################################################################
response_df <- unique(df2[ , c("response_component", "uniq_obs_id")])
response_df <- as.data.frame(table(response_df$response_component))
colnames(response_df) <- c("response_component", "count")
response_df <- response_df[order(response_df$count, decreasing = TRUE), ]
#Write out counts in tab-delimited format
write.table(response_df,
file = logfile_path(logdir, base_filename, "response_component_counts.tab.txt"),
sep = "\t", row.names = FALSE)
# Write out counts in Excel format
write.xlsx(response_df,
file = logfile_path(logdir, base_filename, "response_component_counts.xlsx"),
sheetName = sheet_name, row.names = FALSE)
# Write out counts in RDS format
saveRDS(response_df,
file = logfile_path(logdir, base_filename, "response_component_counts.RDS"))
##########################################################
####  Collect publication titles, dates and abstracts ####
##########################################################
pmids <- unique(df2$publication_reference)
summary_df <- add_to_summary(summary_df, "Unique PMIDs", length(pmids))
# pmids <- 16571413   # for testing
# pmids <- "24336226" # for testing
# pmids <- "23594957" # for testing
if (RENEW_PMIDS || !file.exists(pmid_file)) {
td <- lapply(pmids, pmid_to_title_easy, "PubDate")
titles_and_dates_df <- as.data.frame(rbindlist(td))
save(titles_and_dates_df, file = pmid_file)
} else {
load(file = pmid_file)
}
#############################################################
#### Recreate original spreadsheet with all corrections #####
#############################################################
# Collect multi-valued columns by unique observation ID (each row in original spreadsheet)
uniqIDs                    <- unique(df2$uniq_obs_id)
resp_components_cnt_df     <- data.frame()
resp_components_annotated  <- vector("list", length(uniqIDs))
resp_components_collected  <- vector("list", length(uniqIDs))
resp_components_full_sig   <- vector("list", length(uniqIDs))   # used for cell types only
recreated_template         <- vector("list", length(uniqIDs))
# Some signatures are lost entirely during cleaning
which(!(names(signatures_uniq) %in% uniqIDs))
signatures_uniq <- signatures_uniq[names(signatures_uniq) %in% uniqIDs]
# create data structures needed for mSigDB
if (sheet_type == "GENE" && CREATE_MSIGDB) {
msigdb_empty <- msigdb_intialize()
msigdb_list <- vector("list", length(uniqIDs))
}
for (i in 1:length(uniqIDs)) {
df2tmp <- df2[df2$uniq_obs_id == uniqIDs[i], ]
# Recreate a full signature in one row
base_row <- df2tmp[1, ] # get first row for this uniqID
response_rowname     <- paste(base_row$publication_reference, base_row$subm_obs_id, uniqIDs[i], sep = "_")
response_description <- paste("PMID", base_row$publication_reference, "submission", base_row$subm_obs_id, "row", uniqIDs[i], sep = " ")
# Use the full original set of response components rather than just those
# for which a valid symbol was found.
base_row$response_component_original <- paste(unique(df2tmp$response_component_original), collapse = "; ")
# The "updated" (gene or cell types after fixes) response_components
resp_components_collected[[i]] <- unique(df2tmp$response_component)  # genes or base cell types
if (sheet_type == "GENE") {
base_row$response_component <- paste(unique(df2tmp$response_component), collapse = "; ")
resp_components_annotated[[i]] <- c(response_rowname, response_description, unique(df2tmp$response_component))
# get abstract by PMID
w <- which(titles_and_dates_df$pmid == base_row$publication_reference)
if (length(w) != 1) {
stop(paste("unexpected PMID result: uniqID = ", uniqIDs[i], ", pmid = ", base_row$publication_reference))
}
titles_and_dates_row <- titles_and_dates_df[w, ]
if(CREATE_MSIGDB) {
# Create mSigDB submission row
msigdb_df <- msigdb_process_row(msigdb_empty, df2tmp)
msigdb_list[[i]] <- msigdb_df
}
} else if (sheet_type == "CELLTYPE_FREQUENCY") {
full_sig <- unique(df2tmp$fully_qualified_response_component)
base_row$response_component    <- paste(full_sig, collapse = "; ")
resp_components_full_sig[[i]]  <- full_sig
resp_components_annotated[[i]] <- c(response_rowname, response_description, full_sig)
}
tmp <- data.frame(rowname = response_rowname,
pmid = base_row$publication_reference,
subm_obs_id = base_row$subm_obs_id,
uniq_obs_id = uniqIDs[i],
count = length(resp_components_collected[[i]]))
resp_components_cnt_df <- rbind(resp_components_cnt_df, tmp)
# Reconstitute target_pathogen and exposure_material
base_row$target_pathogen   <- paste(unique(df2tmp$target_pathogen), collapse = "; ")
base_row$exposure_material <- paste(unique(df2tmp$exposure_material), collapse = "; ")
recreated_template[[i]] <- base_row
}
names(resp_components_collected) <- uniqIDs  # name not actually used again?
names(resp_components_full_sig)  <- uniqIDs  # name not actually used again?
names(resp_components_annotated) <- uniqIDs  # name is used for this one.
recreated_template_df <- as.data.frame(rbindlist(recreated_template))  # consolidate to a single data.frame
if(any(colnames(header_rows) != colnames(recreated_template_df))) {
stop("mismatch between header rows and recreated_template_df rows")
}
recreated_template_df <- rbind(header_rows, recreated_template_df)
# Finish up and write out mSigDB submission
if (sheet_type == "GENE" && CREATE_MSIGDB) {
summary_df <- write_msigdb_submission(msigdb_list, summary_df)
}
# Delete unneeded columns from the recreated template
if (sheet_type == "GENE") {
# do nothing
} else if (sheet_type == "CELLTYPE_FREQUENCY") {
delCols <- c("proterm_and_extra", "fully_qualified_response_component")
recreated_template_df <- recreated_template_df[, !colnames(recreated_template_df) %in% delCols]
}
delCols <- c("submission_name", "submission_date", "template_name")
recreated_template_df <- recreated_template_df[, !colnames(recreated_template_df) %in% delCols]
# Write out the recreated upload template in tab-delimited format
write.table(recreated_template_df,
file = logfile_path(logdir, base_filename, "recreated_template.tab.txt"),
sep = "\t", row.names = FALSE)
# Write out the recreated upload template in Excel format
write.xlsx(recreated_template_df,
file = logfile_path(logdir, base_filename, "recreated_template.xlsx"),
sheetName = sheet_name, row.names = FALSE)
# Write out the recreated upload template in RDS format
saveRDS(recreated_template_df,
file = logfile_path(logdir, base_filename, "recreated_template.RDS"))
########################################
##### Prepare submission templates #####
########################################
# Format URLs and handle case of multiple URLs
# Due to length restriction of 255 characters on evidence columns,
# can in practice only return at most two concatenated URLs.
# NOTE - There are special cases in this routine
# NOTE - Update code if new publication URL formats are added
df2$signature_source_url <- format_hipc_urls(df2$signature_source_url)
# for observation summary to read correctly, need to change response_behavior
df2$response_behavior <- sub("positive", "positively", df2$response_behavior, ignore.case = TRUE )
df2$response_behavior <- sub("negative", "negatively", df2$response_behavior, ignore.case = TRUE )
# Remove columns not needed for Dashboard.
recreated_template_df <- recreated_template_df[!colnames(recreated_template_df) %in% c("short_comment", "process_note")]
# do a final check for illegal characters in df2
strict_char_check(df2, "\xa0")  # character 160.  Dashboard loader does not like it.
# check each column of a data frame for the test character
strict_char_check <- function(in_df, testchar) {
l_out <- lapply(colnames(in_df), function(x) {
w <- which(sapply(in_df[x], function(y) {grepl(testchar, y)}))
if(length(w) > 0){
return(paste(x, paste(w, collapse = ", ")))
} else {
return(NULL)
}
})
keep <- !sapply(l_out, is.null)
if(any(keep)) {
return(l_out[keep])
} else {
return(NULL)
}
}
s <- strict_char_check(df2, "\xa0")  # character 160.  Dashboard loader does not like it.
if(!is.null(s)) {
print(paste("for df2, found problems in columns", s))
}
s <- strict_char_check(df2, "\xa0")  # character 160.  Dashboard loader does not like it.
if(!is.null(s)) {
print(paste("for df2, found problems in column", s))
}
s <- strict_char_check(ctf_fixes, "\xa0")  # character 160.  Dashboard loader does not like it.
if(!is.null(s)) {
print(paste("for ctf_fixes, found problems in column (row numbers do not include any header)", s))
}
s <- strict_char_check(df2, "\xa0")  # character 160.  Dashboard loader does not like it.
if(!is.null(s)) {
print(paste("for df2, found problems in column (row numbers do not include any header)", s))
}
